---
title: "Feb 13, over-fitting"
author: "Stephen R. Proulx"
date: "2/10/2025"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rethinking)
source("../helper.R")

```

# Today's objectives:  

* Calculate the fit of a model to data for each data point, averaging over the sampled parameter values   
* Write out a regression model with a polynomial function for the transformation  
* Observing overfitting in polynomial regression models    
* Observe overfitting in categorical models  
* Write out binomial likelihood models with groups of data



## Quantifying the model's fit using posterior simulations
 


Remember our simulated plant data:
```{r}
set.seed(71)
# number of plants
N <- 100

# simulate initial heights
h0 <- rnorm(N,10,2)

# assign treatments and simulate fungus and growth
treatment <- rep( 0:1 , each=N/2 )
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )
h1 <- h0 + rnorm(N, 5 - 3*fungus)

# compose a clean data frame
sim_plant_data <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
```

And our two competing models, one with an effect of treatment and fungus, the other with only treatment.
```{r}
m6.7 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment + bf*fungus,
        a ~ dlnorm( 0 , 0.2 ) ,
        bt ~ dnorm( 0 , 0.5 ),
        bf ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=sim_plant_data )

m6.8 <- quap(
    alist(
        h1 ~ dnorm( mu , sigma ),
        mu <- h0 * p,
        p <- a + bt*treatment,
        a ~ dlnorm( 0 , 0.2 ),
        bt ~ dnorm( 0 , 0.5 ),
        sigma ~ dexp( 1 )
    ), data=sim_plant_data )
```

```{r}
precis(m6.7)

precis(m6.8)
```

We found that in `m6.8`, the treatment has a positive effect on plant growth, but in `m6.7`, treatment has an effect close to zero and equally likely to be positive as negative. Fungus, on the other hand, has a strictly negative effect. This is an example of post-treatment bias, lack of fungus is due to the treatment, but not all treated plants lack fungus, so fungus is more associated with plant growth than treatment. Even more, once you know the fungus status of a plant, learning treatment does little. 


How do these models differ in terms of how they predict the observed growth rates? We can calculate this by measuring the average difference between simulations from our inferred parameters and the actual observed values, and we can do this for every single point in the dataset. We will do this using the mean squared difference between the observation and the simulation, what is commonly used in least-squares regression and in calculating $R^2$.

Summarize the simulations from `m6.7`
```{r}

samples.m6.7 <- link_df(m6.7,sim_plant_data)

summarised.samples.m6.7 <- group_by(samples.m6.7,index) %>%
  summarise(mean.mu=mean(mu),
            lower.mu=quantile(mu,0.1),
            upper.mu=quantile(mu,0.9))%>%
  ungroup() %>%
  left_join(rowid_to_column(sim_plant_data,"index")) %>%
  mutate(res.h1 = (h1-mean.mu)^2 )

```


Summarize the simulations from `m6.8`
```{r}
samples.m6.8 <- link_df(m6.8,sim_plant_data)

summarised.samples.m6.8 <- group_by(samples.m6.8,index) %>%
  summarise(mean.mu=mean(mu),
            lower.mu=quantile(mu,0.1),
            upper.mu=quantile(mu,0.9))%>%
  ungroup() %>%
 left_join(rowid_to_column(sim_plant_data,"index")) %>%
  mutate(res.h1 = (h1-mean.mu)^2 )
```

Now we plot them on the same graph with the more complex model in green.  
```{r}
ggplot(summarised.samples.m6.8, aes(x=index,y=res.h1))+
  geom_point(color="red")  +
  geom_point(data=summarised.samples.m6.7, color="green")
  
```
And to see it a bit more clearly, here are the first 25 points only. In almost each case, the green is closer to 0, meaning better. But not in all cases, individual 7, 10, and 18 get fit way better by the simpler model.
```{r}
ggplot(filter(summarised.samples.m6.8,index<25), aes(x=index,y=res.h1))+
  geom_point(color="red")  +
  geom_point(data=filter(summarised.samples.m6.7,index<25), color="green")
  
```



We can observe that the model with fungus and treatment has dots that tend to be closer to 0 than the treatment only model. This is telling us that the model with both fungus and treatment tends to be better at prediction (in sample) than the model with only treatment. Look back through the data, and check to see which data points are predicted best by the model with only treatment. Why would this be?

## Section 7.1: fitting polynomials and assessing their fit


First we set up a dataframe with the brain volume/mass data.
```{r}
## R code 7.1
  

d <- tibble(species = c( "afarensis","africanus","habilis","boisei",
    "rudolfensis","ergaster","sapiens"), 
    brain=c( 438 , 452 , 612, 521, 752, 871, 1350 ),
    mass = c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 )
    ) %>%
  mutate(mass_std = standardize(mass),
         brain_std = brain/max(brain)) %>%
  rowid_to_column("index")

ggplot(data=d, aes(x=mass_std,y=brain_std)) + 
  geom_point()
```


Now we fit a linear regression, with a little twist. We need to constrain sigma to be positive, and assuming it is log-normally distributed is a good way to do this. 

Both of the following do the same thing. The second one uses a convenient notation for transformations that will come in handy for other applications soon.
```{r}
## R code 7.3
m7.1 <- quap(
    alist(
        brain_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b*mass_std,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d )
```


```{r}
## R code 7.3
m7.1 <- quap(
    alist(
        brain_std ~ dnorm( mu , sigma ),
        mu <- a + b*mass_std,
        log(sigma) <- log_sigma ,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=d )
```


Now we will plot the average and quantiles of the predicted relationship along with the data.
```{r}
samps.7.1 <- link_df(m7.1,data=tibble(mass_std=seq(from=-1, to=1.5, by=0.1)))

summary.7.1 <- samps.7.1 %>%
  group_by(mass_std)%>%
  summarise(mean_mu = mean(mu),
            lower_mu = quantile(mu,0.1),
            upper_mu = quantile(mu,0.9)) %>%
  ungroup() 

ggplot(data=d, aes(x=mass_std,y=brain_std)) + 
  geom_point()+
  geom_ribbon(data=summary.7.1, inherit.aes = FALSE,aes(x=mass_std, ymin=lower_mu,ymax=upper_mu), fill="blue", alpha=0.2)+
  geom_line(data=summary.7.1,aes(x=mass_std,y=mean_mu),color="red")
```


We can add more and more complicated transformation formulas, which could be because we have a specific scientific hypothesis about the relationship between body mass and brain size. But in this case we will just add more and more complex polynomials. 

$$
\mu = \alpha + \beta_1 M + \beta_2 M^2
$$
```{r  }
## R code 7.7
m7.3 <- quap(
    alist(

 
```


Now you write the code for the models that have higher order polynomials up to 5 terms. 
 


For the final model, we need to specify the standard deviation itself or the model won't run properly. Here is the start, you can finish the command. 
```{r  }
## R code 7.9
m7.6 <- quap(
    alist(
        brain_std ~ dnorm( mu , 0.001 ),
       

```




Plot the model fit ranges for each of the models. How do they describe the data?


### overfitting fish

```{r}

load("../Midterm/ClownFishData.RData")
data<- data %>% 
  distinct()%>%
  rowid_to_column("index")
 
```


Fit the proportion of eggs laid
```{r}
m.all <- quap( alist(
  Eggs_Hatched ~ dbinom(Eggs_Laid,mu) ,
  mu ~ dunif(0,1)),
  data=data
)

precis(m.all)
```

It's very certain that, on average, 61% of eggs hatch. Which is just repeating to us that, over the whole dataset, 61% of the eggs did hatch.

```{r}
sum(data$Eggs_Hatched)/sum(data$Eggs_Laid)
```

Let's divide the data up into two groups and fit each group with it's own $\mu$ parameter.
```{r}
data.two <- data %>% mutate(group=ceiling(index/64)) %>% view()
```


I have added some options to quap to give it a list of initial conditions (2 since there are two groups here) and specify some numerical methods. You will want to use these options for the rest of the examples.
```{r}
m.two <- quap( alist(
  Eggs_Hatched ~ dbinom(Eggs_Laid,mu[group]) ,
  mu[group] ~ dunif(0,1)),
  data=data.two,
  start=list( mu = rep(0.6,2)),
  method="Nelder-Mead",
  control=list(maxit=2000)
)


precis(m.two, depth=2)
```

And now visualize the parameters that we get by fitting the two groups separately.
```{r}
mu.samps<-extract.samples(m.two  )$mu%>% as_tibble()


bayesplot::mcmc_intervals(mu.samps) 
```



Now you can divide the data up into 4 groups, or 8 groups, or 16 groups, ... up to 64 groups. What happens as we keep dividing it up into smaller groups?





